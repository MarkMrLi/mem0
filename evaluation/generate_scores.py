import argparse
import json

import pandas as pd

# ç±»åˆ«æ˜ å°„è¡¨
CATEGORY_MAPPING = {
    "1": "multi-hop",
    "2": "temporal", 
    "3": "open-domain",
    "4": "single-hop",
    "5": "adversarial"
}


def generate_scores(input_file="evaluation_metrics.json", output_csv=None, print_terminal=True):
    """ç”Ÿæˆè¯„ä¼°å¾—åˆ†ï¼Œæ”¯æŒCSVè¾“å‡ºå’Œç»ˆç«¯æ‰“å°"""

    # Load the evaluation metrics data
    with open(input_file, "r") as f:
        data = json.load(f)

    # Flatten the data into a list of question items
    all_items = []
    for key in data:
        all_items.extend(data[key])

    # Convert to DataFrame
    df = pd.DataFrame(all_items)

    # å¦‚æœæ•°æ®ä¸­æ²¡æœ‰category_nameåˆ—ï¼Œæ·»åŠ å®ƒ
    if "category_name" not in df.columns:
        df["category_name"] = df["category"].apply(
            lambda x: CATEGORY_MAPPING.get(str(x), f"category_{x}")
        )

    # æŒ‰ç±»åˆ«åç§°åˆ†ç»„
    category_result = df.groupby("category_name").agg({
        "bleu_score": "mean",
        "f1_score": "mean",
        "llm_score": "mean"
    }).round(4)

    # Add count of questions per category
    category_result["count"] = df.groupby("category_name").size()

    # Calculate overall means
    overall_means = df.agg({"bleu_score": "mean", "f1_score": "mean", "llm_score": "mean"}).round(4)

    # è¾“å‡ºCSVæ–‡ä»¶
    if output_csv:
        category_result.to_csv(output_csv)
        print(f"ğŸ“Š å¾—åˆ†å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {output_csv}")

        # ä¹Ÿå¯ä»¥ä¿å­˜å®Œæ•´çš„æ•°æ®
        full_data_csv = output_csv.replace(".csv", "_full_data.csv")
        df.to_csv(full_data_csv, index=False)
        print(f"ğŸ“Š å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°CSVæ–‡ä»¶: {full_data_csv}")

    # æ‰“å°åˆ°ç»ˆç«¯
    if print_terminal:
        print("\nğŸ“Š Mean Scores Per Category:")
        print(category_result)

        print("\nğŸ“ˆ Overall Mean Scores:")
        print(overall_means)

    return category_result, overall_means


def main():
    parser = argparse.ArgumentParser(description="ç”Ÿæˆè¯„ä¼°å¾—åˆ†æŠ¥å‘Š")
    parser.add_argument(
        "--input_file", type=str, default="evaluation_metrics.json",
        help="è¯„ä¼°æŒ‡æ ‡JSONæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output_csv", type=str, default="scores.csv",
        help="è¾“å‡ºCSVæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--no_print", action="store_true", default=False,
        help="ä¸æ‰“å°åˆ°ç»ˆç«¯"
    )

    args = parser.parse_args()

    generate_scores(
        input_file=args.input_file,
        output_csv=args.output_csv,
        print_terminal=not args.no_print
    )


if __name__ == "__main__":
    main()
