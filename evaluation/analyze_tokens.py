#!/usr/bin/env python3
"""
åˆ†æFull-Contextå®éªŒçš„Tokenä½¿ç”¨æƒ…å†µ
"""

import json
import argparse
from collections import defaultdict

def analyze_token_usage(results_file):
    """åˆ†ætokenä½¿ç”¨æƒ…å†µ"""
    print(f"ğŸ“Š åˆ†ææ–‡ä»¶: {results_file}")
    
    with open(results_file, "r") as f:
        data = json.load(f)
    
    # æ”¶é›†æ‰€æœ‰tokenä½¿ç”¨æ•°æ®
    token_data = []
    category_stats = defaultdict(list)
    conversation_stats = defaultdict(list)
    
    for conv_id, questions in data.items():
        for qa in questions:
            item = {
                "conv_id": conv_id,
                "category": qa.get("category", ""),
                "input_tokens": qa.get("input_tokens", 0),
                "output_tokens": qa.get("output_tokens", 0),
                "total_tokens": qa.get("total_tokens", 0),
                "context_chars": qa.get("context_chars", 0),
                "context_length": qa.get("context_length", 0),
                "response_time": qa.get("response_time", 0),
                "question": qa.get("question", ""),
                "response": qa.get("response", "")
            }
            token_data.append(item)
            category_stats[item["category"]].append(item)
            conversation_stats[conv_id].append(item)
    
    if not token_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°tokenä½¿ç”¨æ•°æ®")
        return
    
    # æ€»ä½“ç»Ÿè®¡
    total_input = sum(item["input_tokens"] for item in token_data)
    total_output = sum(item["output_tokens"] for item in token_data)
    total_all = sum(item["total_tokens"] for item in token_data)
    total_chars = sum(item["context_chars"] for item in token_data)
    
    avg_input = total_input / len(token_data)
    avg_output = total_output / len(token_data)
    avg_total = total_all / len(token_data)
    avg_chars = total_chars / len(token_data)
    
    print("=" * 60)
    print("ğŸ“Š Tokenä½¿ç”¨æ€»ä½“ç»Ÿè®¡")
    print("=" * 60)
    print(f"ğŸ“ˆ æ€»é—®é¢˜æ•°: {len(token_data)}")
    print(f"ğŸ”¤ æ€»è¾“å…¥tokens: {total_input:,}")
    print(f"ğŸ”¤ æ€»è¾“å‡ºtokens: {total_output:,}")
    print(f"ğŸ”¤ æ€»tokens: {total_all:,}")
    print(f"ğŸ“ å¹³å‡è¾“å…¥tokens: {avg_input:.1f}")
    print(f"ğŸ“ å¹³å‡è¾“å‡ºtokens: {avg_output:.1f}")
    print(f"ğŸ“ å¹³å‡æ€»tokens: {avg_total:.1f}")
    print(f"ğŸ“„ å¹³å‡ä¸Šä¸‹æ–‡å­—ç¬¦æ•°: {avg_chars:.1f}")
    
    if avg_chars > 0 and avg_input > 0:
        chars_per_token = avg_chars / avg_input
        print(f"ğŸ“Š å­—ç¬¦/tokenæ¯”ä¾‹: {chars_per_token:.2f}")
    
    # æå€¼ç»Ÿè®¡
    max_input = max(token_data, key=lambda x: x["input_tokens"])
    min_input = min(token_data, key=lambda x: x["input_tokens"])
    max_output = max(token_data, key=lambda x: x["output_tokens"])
    min_output = min(token_data, key=lambda x: x["output_tokens"])
    max_total = max(token_data, key=lambda x: x["total_tokens"])
    min_total = min(token_data, key=lambda x: x["total_tokens"])
    
    print("\n" + "=" * 60)
    print("ğŸ† Tokenæå€¼ç»Ÿè®¡")
    print("=" * 60)
    print(f"ğŸ“ˆ æœ€å¤§è¾“å…¥tokens: {max_input['input_tokens']:,}")
    print(f"   Question: {max_input['question'][:80]}...")
    print(f"ğŸ“‰ æœ€å°è¾“å…¥tokens: {min_input['input_tokens']:,}")
    print(f"   Question: {min_input['question'][:80]}...")
    print(f"ğŸ“ˆ æœ€å¤§è¾“å‡ºtokens: {max_output['output_tokens']:,}")
    print(f"   Question: {max_output['question'][:80]}...")
    print(f"ğŸ“‰ æœ€å°è¾“å‡ºtokens: {min_output['output_tokens']:,}")
    print(f"   Question: {min_output['question'][:80]}...")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“‚ æŒ‰Categoryç»Ÿè®¡")
    print("=" * 60)
    
    category_mapping = {
        "1": "multi-hop",
        "2": "temporal",
        "3": "open-domain", 
        "4": "single-hop",
        "5": "adversarial"
    }
    
    for category, items in sorted(category_stats.items()):
        cat_name = category_mapping.get(category, f"category_{category}")
        cat_total_input = sum(item["input_tokens"] for item in items)
        cat_total_output = sum(item["output_tokens"] for item in items)
        cat_avg_total = cat_total_input + cat_total_output / len(items)
        
        print(f"\nğŸ“Š {cat_name} (category: {category})")
        print(f"   é—®é¢˜æ•°: {len(items)}")
        print(f"   å¹³å‡è¾“å…¥tokens: {cat_total_input / len(items):.1f}")
        print(f"   å¹³å‡è¾“å‡ºtokens: {cat_total_output / len(items):.1f}")
        print(f"   å¹³å‡æ€»tokens: {(cat_total_input + cat_total_output) / len(items):.1f}")
    
    # Tokenåˆ†å¸ƒ
    print("\n" + "=" * 60)
    print("ğŸ“Š Tokenä½¿ç”¨åˆ†å¸ƒ")
    print("=" * 60)
    
    input_ranges = [
        (0, 1000, "0-1K"),
        (1000, 2000, "1K-2K"),
        (2000, 4000, "2K-4K"),
        (4000, 8000, "4K-8K"),
        (8000, 16000, "8K-16K"),
        (16000, float('inf'), "16K+")
    ]
    
    print("è¾“å…¥tokensåˆ†å¸ƒ:")
    for min_tokens, max_tokens, label in input_ranges:
        if max_tokens == float('inf'):
            count = sum(1 for item in token_data if item["input_tokens"] >= min_tokens)
        else:
            count = sum(1 for item in token_data if min_tokens <= item["input_tokens"] < max_tokens)
        percentage = (count / len(token_data)) * 100
        bar = "â–ˆ" * int(percentage / 2)
        print(f"{label:>8}: {count:>4} ({percentage:>5.1f}%) {bar}")
    
    print("\nè¾“å‡ºtokensåˆ†å¸ƒ:")
    output_ranges = [
        (0, 50, "0-50"),
        (50, 100, "50-100"),
        (100, 200, "100-200"),
        (200, 500, "200-500"),
        (500, float('inf'), "500+")
    ]
    
    for min_tokens, max_tokens, label in output_ranges:
        if max_tokens == float('inf'):
            count = sum(1 for item in token_data if item["output_tokens"] >= min_tokens)
        else:
            count = sum(1 for item in token_data if min_tokens <= item["output_tokens"] < max_tokens)
        percentage = (count / len(token_data)) * 100
        bar = "â–ˆ" * int(percentage / 2)
        print(f"{label:>8}: {count:>4} ({percentage:>5.1f}%) {bar}")
    
    # æ•ˆç‡åˆ†æ
    print("\n" + "=" * 60)
    print("âš¡ æ•ˆç‡åˆ†æ")
    print("=" * 60)
    
    avg_response_time = sum(item["response_time"] for item in token_data) / len(token_data)
    tokens_per_second = total_all / sum(item["response_time"] for item in token_data)
    
    print(f"â±ï¸  å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
    print(f"ğŸš€ æ€»å¤„ç†é€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
    
    # æˆæœ¬ä¼°ç®— (å‡è®¾GPT-4ä»·æ ¼)
    print("\n" + "=" * 60)
    print("ğŸ’° æˆæœ¬ä¼°ç®— (GPT-4å®šä»·å‚è€ƒ)")
    print("=" * 60)
    
    # GPT-4å®šä»·: è¾“å…¥ $0.03/1K tokens, è¾“å‡º $0.06/1K tokens
    input_cost = (total_input / 1000) * 0.03
    output_cost = (total_output / 1000) * 0.06
    total_cost = input_cost + output_cost
    
    print(f"ğŸ’µ è¾“å…¥æˆæœ¬: ${input_cost:.4f}")
    print(f"ğŸ’µ è¾“å‡ºæˆæœ¬: ${output_cost:.4f}")
    print(f"ğŸ’µ æ€»æˆæœ¬: ${total_cost:.4f}")
    print(f"ğŸ’µ å¹³å‡æ¯é—®é¢˜æˆæœ¬: ${total_cost / len(token_data):.6f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="åˆ†æFull-Contextå®éªŒçš„Tokenä½¿ç”¨æƒ…å†µ")
    parser.add_argument("--input_file", type=str, default="results/full_context_results.json",
                       help="Full-Contextå®éªŒç»“æœæ–‡ä»¶")
    
    args = parser.parse_args()
    
    analyze_token_usage(args.input_file)