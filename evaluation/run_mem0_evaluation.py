#!/usr/bin/env python3
import argparse
import json
import os
import sys
import threading  # 1. ç§»åŠ¨åˆ°è¿™é‡Œï¼Œç¡®ä¿å…¨å±€å¯ç”¨
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

from dotenv import load_dotenv
from tqdm import tqdm

# æ³¨æ„ï¼šè¯·ç¡®ä¿è¿™äº›æ¨¡å—åœ¨ä½ çš„è·¯å¾„ä¸­
try:
    from src.memzero.search import MemorySearch
    from metrics.llm_judge import evaluate_llm_judge
    from metrics.utils import calculate_bleu_scores, calculate_metrics
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")

load_dotenv()

# ç±»åˆ«æ˜ å°„è¡¨
CATEGORY_MAPPING = {
    "1": "multi-hop",
    "2": "temporal",     # ä¿®æ­£ï¼šå°†ç©ºå­—ç¬¦ä¸²æ”¹ä¸º "temporal"
    "3": "open-domain",
    "4": "single-hop",
    "5": "adversarial"
}

def process_item(item_data):
    """å¤„ç†å•ä¸ªè¯„ä¼°é¡¹ç›®"""
    k, v = item_data
    local_results = defaultdict(list)

    for item in v:
        gt_answer = str(item.get("answer", ""))
        pred_answer = str(item.get("response", ""))
        category = str(item.get("category", ""))
        question = str(item.get("question", ""))

        if category == "5":
            continue

        metrics = calculate_metrics(pred_answer, gt_answer)
        bleu_scores = calculate_bleu_scores(pred_answer, gt_answer)
        llm_score = evaluate_llm_judge(question, gt_answer, pred_answer)

        category_name = CATEGORY_MAPPING.get(category, f"category_{category}")

        local_results[k].append({
            "question": question,
            "answer": gt_answer,
            "response": pred_answer,
            "category": category,
            "category_name": category_name,
            "bleu_score": bleu_scores["bleu1"],
            "f1_score": metrics["f1"],
            "llm_score": llm_score,
        })

    return local_results

def run_evaluation(data_file, output_folder, top_k=30, filter_memories=False, is_graph=False, max_workers=10):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"mem0_eval_top{top_k}_filter{filter_memories}_graph{is_graph}_{timestamp}"
    experiment_dir = os.path.join("/home/suma2/repo/mem0/evaluation/results/mem0_eval_top30_filterFalse_graphFalse_20260102_143036")
    os.makedirs(experiment_dir, exist_ok=True)

    print(f"ğŸš€ å¼€å§‹å®Œæ•´çš„mem0è¯„ä¼°æµç¨‹")
    print(f"ğŸ“ å®éªŒç»“æœå°†ä¿å­˜åˆ°: {experiment_dir}")

    # Step 1: è¿è¡Œæœç´¢
    # print(f"\nğŸ” Step 1: è¿è¡Œè®°å¿†æœç´¢...")
    search_results_file = os.path.join(experiment_dir, "search_results.json")
    # memory_searcher = MemorySearch(
    #     output_path=search_results_file,
    #     top_k=top_k,
    #     filter_memories=filter_memories,
    #     is_graph=is_graph
    # )
    # memory_searcher.process_data_file(data_file)
    # print(f"âœ… æœç´¢å®Œæˆ")

    # Step 2: è¿è¡Œè¯„ä¼°
    print(f"\nğŸ“Š Step 2: ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡...")
    eval_metrics_file = os.path.join(experiment_dir, "evaluation_metrics.json")

    with open(search_results_file, "r") as f:
        search_data = json.load(f)

    results = defaultdict(list)
    results_lock = threading.Lock()

    # ä¼˜åŒ–åçš„å¤šçº¿ç¨‹è¯„ä¼°é€»è¾‘
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # å°† items() è½¬æ¢ä¸ºåˆ—è¡¨è¿›è¡Œè¿­ä»£
        items_to_process = list(search_data.items())
        
        # ä½¿ç”¨ list() åŒ…è£… map ä»¥ä¾¿ç«‹å³å¼€å§‹æ‰§è¡Œï¼Œå¹¶åœ¨ tqdm ä¸­æ˜¾ç¤ºè¿›åº¦
        for local_res in tqdm(executor.map(process_item, items_to_process), 
                             total=len(items_to_process), 
                             desc="å¤„ç†è¯„ä¼°"):
            with results_lock:
                for k, items in local_res.items():
                    results[k].extend(items)

    # 2. ä¿®æ­£è¯­æ³•é”™è¯¯ï¼šjson.() -> json.dump()
    with open(eval_metrics_file, "w") as f:
        json.dump(results, f, indent=4, ensure_ascii=False)
    print(f"âœ… è¯„ä¼°æŒ‡æ ‡ä¿å­˜å®Œæˆ")

    # Step 3: ç”Ÿæˆå¾—åˆ†æŠ¥å‘Š
    print(f"\nğŸ“ˆ Step 3: ç”Ÿæˆå¾—åˆ†æŠ¥å‘Š...")
    scores_csv_file = os.path.join(experiment_dir, "scores.csv")

    all_items = []
    for key in results:
        all_items.extend(results[key])

    import pandas as pd
    df = pd.DataFrame(all_items)

    category_results = df.groupby("category_name").agg({
        "bleu_score": "mean",
        "f1_score": "mean",
        "llm_score": "mean"
    }).round(4)
    category_results["count"] = df.groupby("category_name").size()

    overall_means = df.agg({"bleu_score": "mean", "f1_score": "mean", "llm_score": "mean"}).round(4)
    category_results.to_csv(scores_csv_file)

    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        "experiment_name": experiment_name,
        "parameters": {"top_k": top_k, "is_graph": is_graph},
        "overall_scores": overall_means.to_dict()
    }
    with open(os.path.join(experiment_dir, "metadata.json"), "w") as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    print("\nğŸ“Š è¯„ä¼°å®Œæˆï¼")
    print(category_results)
    return experiment_dir

def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œå®Œæ•´çš„mem0è¯„ä¼°æµç¨‹")
    parser.add_argument("--data_file", type=str, default="dataset/locomo10.json")
    parser.add_argument("--output_folder", type=str, default="results/")
    parser.add_argument("--top_k", type=int, default=30)
    parser.add_argument("--filter_memories", action="store_true")
    parser.add_argument("--is_graph", action="store_true")
    parser.add_argument("--max_workers", type=int, default=10)

    args = parser.parse_args()

    run_evaluation(
        data_file=args.data_file,
        output_folder=args.output_folder,
        top_k=args.top_k,
        filter_memories=args.filter_memories,
        is_graph=args.is_graph,
        max_workers=args.max_workers
    )

if __name__ == "__main__":
    main()