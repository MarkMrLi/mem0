#!/usr/bin/env python3
"""
æ•´åˆçš„mem0è¯„ä¼°æµç¨‹è„šæœ¬
å°†searchã€evalsã€generate_scoresä¸‰ä¸ªæ­¥éª¤åˆå¹¶ä¸ºä¸€ä¸ªæµç¨‹
"""

import argparse
import json
import os
import sys
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime

from dotenv import load_dotenv
from tqdm import tqdm

from src.memzero.search import MemorySearch
from metrics.llm_judge import evaluate_llm_judge
from metrics.utils import calculate_bleu_scores, calculate_metrics

load_dotenv()

# ç±»åˆ«æ˜ å°„è¡¨
CATEGORY_MAPPING = {
    "1": "multi-hop",
    "2": "temporal", 
    "3": "open-domain",
    "4": "single-hop",
    "5": "adversarial"
}


def process_item(item_data):
    """å¤„ç†å•ä¸ªè¯„ä¼°é¡¹ç›®ï¼Œæ¥è‡ªevals.pyçš„é€»è¾‘"""
    k, v = item_data
    local_results = defaultdict(list)

    for item in v:
        gt_answer = str(item["answer"])
        pred_answer = str(item["response"])
        category = str(item["category"])
        question = str(item["question"])

        # Skip category 5 (adversarial)
        if category == "5":
            continue

        metrics = calculate_metrics(pred_answer, gt_answer)
        bleu_scores = calculate_bleu_scores(pred_answer, gt_answer)
        llm_score = evaluate_llm_judge(question, gt_answer, pred_answer)

        # ä½¿ç”¨ç±»åˆ«åç§°è€Œä¸æ˜¯æ•°å­—
        category_name = CATEGORY_MAPPING.get(category, f"category_{category}")

        local_results[k].append(
            {
                "question": question,
                "answer": gt_answer,
                "response": pred_answer,
                "category": category,
                "category_name": category_name,
                "bleu_score": bleu_scores["bleu1"],
                "f1_score": metrics["f1"],
                "llm_score": llm_score,
            }
        )

    return local_results


def run_evaluation(data_file, output_folder, top_k=30, filter_memories=False, is_graph=False, max_workers=10):
    """è¿è¡Œå®Œæ•´çš„è¯„ä¼°æµç¨‹"""

    # åˆ›å»ºæ—¶é—´æˆ³ç”¨äºç»“æœç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    experiment_name = f"mem0_eval_top{top_k}_filter{filter_memories}_graph{is_graph}_{timestamp}"

    # åˆ›å»ºå®éªŒç»“æœç›®å½•
    experiment_dir = os.path.join(output_folder, experiment_name)
    os.makedirs(experiment_dir, exist_ok=True)

    print(f"ğŸš€ å¼€å§‹å®Œæ•´çš„mem0è¯„ä¼°æµç¨‹")
    print(f"ğŸ“ å®éªŒç»“æœå°†ä¿å­˜åˆ°: {experiment_dir}")

    # Step 1: è¿è¡Œæœç´¢
    print(f"\nğŸ” Step 1: è¿è¡Œè®°å¿†æœç´¢...")
    search_results_file = os.path.join(experiment_dir, "search_results.json")

    memory_searcher = MemorySearch(
        output_path=search_results_file,
        top_k=top_k,
        filter_memories=filter_memories,
        is_graph=is_graph
    )
    memory_searcher.process_data_file(data_file)
    print(f"âœ… æœç´¢å®Œæˆï¼Œç»“æœä¿å­˜åˆ°: {search_results_file}")

    # Step 2: è¿è¡Œè¯„ä¼°
    print(f"\nğŸ“Š Step 2: ç”Ÿæˆè¯„ä¼°æŒ‡æ ‡...")
    eval_metrics_file = os.path.join(experiment_dir, "evaluation_metrics.json")

    with open(search_results_file, "r") as f:
        search_data = json.load(f)

    results = defaultdict(list)
    results_lock = threading.Lock()

    # Use ThreadPoolExecutor with specified workers
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(process_item, item_data) for item_data in search_data.items()]

        for future in tqdm(ThreadPoolExecutor(max_workers=max_workers).map(lambda f: f.result(), futures),
                          total=len(futures), desc="å¤„ç†è¯„ä¼°"):
            local_results = future.result()
            with results_lock:
                for k, items in local_results.items():
                    results[k].extend(items)

    # Save evaluation metrics
    with open(eval_metrics_file, "w") as f:
        json.dump(results, f, indent=4)
    print(f"âœ… è¯„ä¼°å®Œæˆï¼ŒæŒ‡æ ‡ä¿å­˜åˆ°: {eval_metrics_file}")

    # Step 3: ç”Ÿæˆå¾—åˆ†å¹¶ä¿å­˜CSV
    print(f"\nğŸ“ˆ Step 3: ç”Ÿæˆå¾—åˆ†æŠ¥å‘Š...")
    scores_csv_file = os.path.join(experiment_dir, "scores.csv")

    # Flatten the data into a list of question items
    all_items = []
    for key in results:
        all_items.extend(results[key])

    # Convert to DataFrame and save as CSV
    import pandas as pd

    df = pd.DataFrame(all_items)

    # æŒ‰ç±»åˆ«åç§°åˆ†ç»„è€Œä¸æ˜¯æ•°å­—
    category_results = df.groupby("category_name").agg({
        "bleu_score": "mean",
        "f1_score": "mean",
        "llm_score": "mean"
    }).round(4)

    # Add count of questions per category
    category_results["count"] = df.groupby("category_name").size()

    # Calculate overall means
    overall_means = df.agg({"bleu_score": "mean", "f1_score": "mean", "llm_score": "mean"}).round(4)

    # Save results to CSV
    category_results.to_csv(scores_csv_file)
    print(f"âœ… å¾—åˆ†æŠ¥å‘Šä¿å­˜åˆ°: {scores_csv_file}")

    # æ‰“å°ç»“æœåˆ°ç»ˆç«¯
    print("\nğŸ“Š è¯„ä¼°ç»“æœ:")
    print("\nå„ç±»åˆ«å¹³å‡å¾—åˆ†:")
    print(category_results)

    print("\næ€»ä½“å¹³å‡å¾—åˆ†:")
    print(overall_means)

    # åˆ›å»ºå®éªŒå…ƒæ•°æ®æ–‡ä»¶
    metadata = {
        "experiment_name": experiment_name,
        "timestamp": timestamp,
        "parameters": {
            "data_file": data_file,
            "top_k": top_k,
            "filter_memories": filter_memories,
            "is_graph": is_graph,
            "max_workers": max_workers
        },
        "files": {
            "search_results": search_results_file,
            "evaluation_metrics": eval_metrics_file,
            "scores_csv": scores_csv_file
        },
        "overall_scores": overall_means.to_dict()
    }

    metadata_file = os.path.join(experiment_dir, "metadata.json")
    with open(metadata_file, "w") as f:
        json.dump(metadata, f, indent=4)

    print(f"\nğŸ¯ å®éªŒå®Œæˆï¼æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {experiment_dir}")
    print(f"ğŸ“‹ å®éªŒå…ƒæ•°æ®: {metadata_file}")

    return experiment_dir


def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œå®Œæ•´çš„mem0è¯„ä¼°æµç¨‹")
    parser.add_argument(
        "--data_file", type=str, default="dataset/locomo10.json",
        help="æ•°æ®é›†æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output_folder", type=str, default="results/",
        help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„"
    )
    parser.add_argument(
        "--top_k", type=int, default=30,
        help="æ£€ç´¢çš„è®°å¿†æ•°é‡"
    )
    parser.add_argument(
        "--filter_memories", action="store_true", default=False,
        help="æ˜¯å¦è¿‡æ»¤è®°å¿†"
    )
    parser.add_argument(
        "--is_graph", action="store_true", default=False,
        help="æ˜¯å¦ä½¿ç”¨å›¾è°±æœç´¢"
    )
    parser.add_argument(
        "--max_workers", type=int, default=10,
        help="æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°"
    )

    args = parser.parse_args()

    # å¯¼å…¥threadingä»¥æ”¯æŒThreadPoolExecutor
    import threading

    run_evaluation(
        data_file=args.data_file,
        output_folder=args.output_folder,
        top_k=args.top_k,
        filter_memories=args.filter_memories,
        is_graph=args.is_graph,
        max_workers=args.max_workers
    )


if __name__ == "__main__":
    main()